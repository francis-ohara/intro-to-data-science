{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d3a36c",
   "metadata": {},
   "source": [
    "# Research Topics\n",
    "1. Types of Data types\n",
    "2. Sample and estimated\n",
    "3. mean, variance, and standard deviation\n",
    "4. Linear transformation of means\n",
    "5. linear transformation of variance\n",
    "6. Range\n",
    "7. Quartiles\n",
    "8. Percentiles\n",
    "9. Covariance and Pearson’s rank correlation\n",
    "10. Handling Missing values\n",
    "11. Detecting outliers\n",
    "12. Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc6642",
   "metadata": {},
   "source": [
    "## 1. Types of Data\n",
    "There are two main types of data encountered in data science:\n",
    "1. Qualitative Data\n",
    "2. Quantitative Data\n",
    "\n",
    "### Qualitative Data\n",
    "Qualitative data or **categorical data**, refers to data that fits into distinct categories and which is non-numerical in nature.\n",
    "E.g. Data about hair color(black, blonde, brunette, red) or sex(male, female).\n",
    "\n",
    "There are two kinds of qualitative data:\n",
    "1. **Nominal**: This refers to qualitative data that follows a specific order or ranking. Certain values can be said to be greater than other values.\n",
    "E.g. test grade = \\[A, B, C, D, E], socioeconomic status = \\[low, middle, high]\n",
    "\n",
    "2. **Ordinal**: This refers to qualitative data that does not follow any specific order or ranking. No value can be said to be greater than another value.\n",
    "E.g. eye color = \\[blue, black, brown, grey], weather = \\[cloudy, sunny, windy, snowy]\n",
    "\n",
    "\n",
    "### Quantitative Data\n",
    "Quantitative data or **numerical data** refers to data that is numerical in nature or which is expressed as numerical values.\n",
    "E.g. temperature = \\[101, 37.5, 25, 0]\n",
    "\n",
    "There are to kinds of quantitative data:\n",
    "1. **Discrete**: This refers to numerical data that can only take up a limited number of values over any specific numeric interval. Such data is typically obtained by counting.\n",
    "E.g. age = \\[18, 9, 25, 88], number of students in different classes = \\[15, 20, 17, 101]\n",
    "2. **Continuous**: This refers to data that can take up an uncountable number of values between any specific numeric interval. Such data is typically obtained by measuring.\n",
    "E.g. Height of person = \\[5, 5.5, 5.59, 5.5559, 9.23456783]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f266578",
   "metadata": {},
   "source": [
    "## 2. Sampling and Estimation\n",
    "### What is sampling?\n",
    "In statistical studies, the **population** refers to the total number of observations obtained for the study.\n",
    "Due to time, cost, and other constraints, it is difficult to collect data from every element of the population for studying.\n",
    "To solve this problem, a subset of the population, referred to as the **sample** is selected and observed, and based on data obtained from the sample, an estimate is made concerning the entire population.\n",
    "\n",
    "Sampling is therefore the act of taking a part or a portion of a population to represent the entire population during a research study. The sample is the group of individuals who will actually participate in the research. These people are only a subset of the total population the study wants to obtain information about.\n",
    "\n",
    "There are two types of sampling methods:\n",
    "1. Probability sampling methods\n",
    "2. Non-Probability sampling methods\n",
    "\n",
    "**Probability Sampling Methods**\n",
    "These methods involve random selection and allow you to make strong statistical inferences about the whole group.\n",
    "The different probability sampling methods are:\n",
    "1. *Simple Random Sample*: In a simple random sample, every member of the population has an equal chance of being selected. Your sampling frame should include the whole population.With this type of sampling tools like simple random number generator will be needed.\n",
    "2. *Systematic sampling*: Systematic sampling is similar to simple random sampling, but it is usually slightly easier to conduct. Every member of the population is listed with a number, but instead of randomly generating numbers, individuals are chosen at regular intervals.\n",
    "3. *Stratified sampling*: Stratified sampling involves dividing the population into subpopulations that may differ in important ways. It allows you draw more precise conclusions by ensuring that every subgroup is properly represented in the sample.\n",
    "4. *Cluster sampling*: Cluster sampling also involves dividing the population into subgroups, but each subgroup should have similar characteristics to the whole sample. Instead of sampling individuals from each subgroup, you randomly select entire subgroups.\n",
    "\n",
    "**Non-Probability Sampling**\n",
    "These methods involve non-random selection based on convenience or other criteria, allowing you to easily collect data.\n",
    "The different non-probability sampling methods include:\n",
    "1. *Convenience sampling*: A convenience sample includes the individuals who are most accessible to the researcher.\n",
    "2. *Voluntary response sampling*: Similar to a convenience sample, a voluntary response sample is mainly based on ease of access. Instead of the researcher choosing participants and directly contacting them, people volunteer themselves.\n",
    "3. *Purposive sampling*: This type of sampling, also known as judgment sampling, involves the researcher using their expertise to select a sample that is most useful to the purposes of the research.\n",
    "4. *Snowball sampling*: If the population is hard to access, snowball sampling can be used to recruit participants via other participants. The number of people you have access to “snowballs” as you get in contact with more people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2520349",
   "metadata": {},
   "source": [
    "\n",
    "### What is Estimation?\n",
    "As alluded to earlier, estimation in statistics is any of the numerous procedures used to calculate the value of some property of a population from observations of a sample drawn from the population.\n",
    "\n",
    "**Types of Estimation**\n",
    "1. *Point Estimate*: A point estimate of a population parameter is a single value most likely to express the value of the parameter.\n",
    "E.g. Based on the sample, we can estimate the mean age of a population to be 17.5 years.\n",
    "2. *Interval Estimate*: An interval estimate is defined by two numbers, between which a population parameter is said to lie.\n",
    "E.g. Based on the sample, we can estimate the mean age of a population to lie between 16 years and 18 years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8eb6c9",
   "metadata": {},
   "source": [
    "## 3. Mean, Variance, and Standard Deviation\n",
    "The mean, variance, and standard deviation are key features used to describe a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412c0c0",
   "metadata": {},
   "source": [
    "### Mean\n",
    "The mean of is a measure of central tendency in descriptive statistics which shows the average value of a characteristic in a given statistical sample.\n",
    "In simple terms, the mean is the average value of a particular data set.\n",
    "\n",
    "Formula to calculate the arithmetic mean for an individual data series:\n",
    "$$\\bar{x} = \\frac{x_1 + x_2 + ... + x_i}{n}$$\n",
    "where n is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a5676",
   "metadata": {},
   "source": [
    "### Variance\n",
    "Variance is another statistical feature of a data set that measures the spread or dispersion of values throughout the data set.\n",
    "It essentially tries to measure the difference between each data value and the mean of the entire data set.\n",
    "It is therefore defined in terms of the mean and can be calculated using the following formula:\n",
    "$$\\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac87f7",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "The standard deviation of a data set measures how much the values within the data set differ from the mean or average.\n",
    "It is therefore a measure of how spread apart or dispersed a data set is.\n",
    "The higher the standard deviation of a data set, the more spread apart the values within it are, and the lower the standard deviation of a data set, the more condensed the values within it are.\n",
    "\n",
    "The formula for finding the standard deviation of a data set is:\n",
    " $$\\sigma = \\sqrt{\\frac{\\sum (x_i - \\mu)^2}{N}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd5ce2",
   "metadata": {},
   "source": [
    "## 4. Linear Transformation of Means\n",
    "A linear transformation (or simply transformation, sometimes called linear map) is a mapping between two vector spaces:\n",
    "it takes a vector as input and transforms it into a new output vector.\n",
    " A function is said to be linear if the properties of additivity and scalar multiplication are preserved, that is, the same result is obtained if these operations are done before or after the transformation.\n",
    " Linear functions are synonymously called linear transformations.\n",
    "\n",
    "\n",
    "A linear transformation changes each original value in the data set $x$ into the new value $x_{new}$ given by an equation of the form\n",
    "\n",
    "$$x_{new} = a + bx$$\n",
    "\n",
    "For example, given a data set `lengths = [1, 2, 3, 4, 5]`, a linear transformation of lengths could be the addition of a constant 2 to each data value, so that $\\text{lengths}_{new}$` = [3, 4, 5, 6, 7]`.\n",
    "\n",
    "This is an example of a linear transformation because there exists a linear relationship between the old data set and the new data set as in the equation stated above with $a = 2$ and $b = 1$.\n",
    "\n",
    "An additional example would be to increase each value in the original data set by $10%$. For the same data set above, $\\text{lengths}_{new}$` = [1.1, 2.2, 3.3, 4.4, 5.5]`.\n",
    "We can confirm that this is also a linear transformation because there exists a linear relationship between the original data set and the new data set with $a = 0$ and $b = 1.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059b5be",
   "metadata": {},
   "source": [
    "**Effect of Linear Transformation on Mean**\n",
    "Now that we understand what a linear transformation is, it is worth asking what effect a linear transformation has on the mean of the data set.\n",
    "If a linear transformation is performed on a data set $x_{new} = a + bx$ where $x$ is the original data set and $x_{new}$ is the new data set the following occurs:\n",
    " - The mean of $x_{new}$ increases by $a$\n",
    " - The mean of $x_{new}$ increases by a factor of $b$\n",
    "\n",
    "The following code cell demonstrates this phenomenon for  a data set `ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee94cc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# before linear transformation\n",
    "ids = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # original data set\n",
    "mean = ids.mean()  # original mean\n",
    "print(\"Data set Before:\", ids)\n",
    "print(\"Mean Before:\", mean)\n",
    "\n",
    "# linear transformation\n",
    "ids = 3 + (4 * ids)\n",
    "\n",
    "# after linear transformation\n",
    "mean = ids.mean()  # transformed mean\n",
    "print(\"Data set After:\", ids)\n",
    "print(\"Mean After:\", mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e9118",
   "metadata": {},
   "source": [
    "From the results above, when the data set was linearly transformed by multiplying each data value by 4 and adding 3, the mean of the transformed data set becomes the mean of the original data set multiplied by 4 and incremented by 3.\n",
    "\n",
    "This confirms the effect of linear transformation on the mean of a data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5b84e",
   "metadata": {},
   "source": [
    "## 5. Linear Transformation of Variance\n",
    "As mentioned earlier, a linear transformation produces a new data set which has a linear relationship to the original.\n",
    "The variance is a measure of variability. It is calculated by taking the average of squared deviations from the mean. Variance tells you the degree of spread in your data set. The more spread the data, the larger the variance is in relation to the mean.\n",
    "\n",
    "**Effect of Linear Transformation on Variance**\n",
    "As stated in the variance formula above, variance measures the difference between each data value and the mean of the data set.\n",
    "\n",
    " - If the linear transformation only involves the addition of a constant to each value in the data set, the variance of the transformed data set remains the same (because each value is increased by the same amount).\n",
    "\n",
    " - However, if the linear transformation involves the multiplication of the original data set by a factor, the variance of the new data set will increase by the factor.\n",
    "\n",
    "\n",
    "The code cell below demonstrates this phenomenon for the data set `ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# before linear transformation\n",
    "ids = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # original data set\n",
    "variance = ids.var()\n",
    "print(\"Data Set Before:\", ids)\n",
    "print(\"Variance Before:\", variance)\n",
    "\n",
    "# linear transformation involving multiplication by factor\n",
    "new_ids_1 = 0 + (4 * ids)\n",
    "\n",
    "# linear transformation involving addition of constant\n",
    "new_ids_2 = 3 + (1 * ids)\n",
    "\n",
    "print(\n",
    "    \"\\nAfter Linear Transformation Involving Addition of Constant\\n----------------------------------------------------------\")\n",
    "variance = new_ids_2.var()\n",
    "print(\"Data Set After:\", new_ids_2)\n",
    "print(\"Variance After:\", variance)\n",
    "\n",
    "# after linear transformation\n",
    "print(\n",
    "    \"\\nAfter Linear Transformation Involving Multiplication by Factor\\n---------------------------------------------------------------\")\n",
    "variance = new_ids_1.var()\n",
    "print(\"Data Set After:\", new_ids_1)\n",
    "print(\"Variance After:\", variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8262d6",
   "metadata": {},
   "source": [
    "As expected, the variance is not affected when the linear transformation only involves the addition of a constant.\n",
    "However, when the linear transformation involves multiplication by a factor, the variance of the data set is increased by the same factor (`4` in the example above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e3cbc",
   "metadata": {},
   "source": [
    "## 6. Range\n",
    "The range of a data set refers to the difference between the largest and smallest value in the data set.\n",
    "It describes how well the central tendency represents the data. If the range is large, the central tendency is not as representative of the data as it would be if the range was small.\n",
    "\n",
    "Example: In `{4, 6, 9, 3, 7}` the lowest value is `3`, and the highest is `9`. So the range is `9 − 3 = 6`.\n",
    "The following code cell demonstrates the calculation of the range of a data set `numbers = [12, 4, 56, 8, 32]` with an expected range of `56 - 4 = 52`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numbers = np.array([12, 4, 56, 8, 32])\n",
    "range_n = numbers.ptp()  # calculating range\n",
    "print(range_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82f476",
   "metadata": {},
   "source": [
    "## 7. Quartiles\n",
    "A quartile is a statistical term that describes a division of observations into four defined intervals based on the values of the data and how they compare to the entire set of observations.\n",
    "<br>\n",
    "### How Quartiles work\n",
    "There are three quartile values—a lower quartile, median, and upper quartile—to divide the data set into four ranges, each containing 25% of the data points. The lower quartile, or first quartile, is denoted as Q1 and is the middle number that falls between the smallest value of the dataset and the median. The second quartile, Q2, is also the median. The upper or third quartile, denoted as Q3, is the central point that lies between the median and the highest number of the distribution.\n",
    "Now, we can map out the four groups formed from the quartiles. The first group of values contains the smallest number up to Q1; the second group includes Q1 to the median; the third set is the median to Q3; the fourth category comprises Q3 to the highest data point of the entire set.\n",
    "Each interval contains 25% of the total observations. Generally, the data is arranged from smallest to largest:\n",
    "\n",
    "- First interval: The set of data points between the minimum value and the first quartile.\n",
    "- Second interval: The set of data points between the lower quartile and the median.\n",
    "- Third interval: The set of data between the median and the upper quartile.\n",
    "- Fourth interval: The set of data points between the upper quartile and the maximum value of the data set.\n",
    "\n",
    "\n",
    "### Example of Quartile\n",
    "Suppose the distribution of math scores in a class of 19 students in ascending order is:\n",
    "\n",
    "59, 60, 65, 65, 68, 69, 70, 72, 75, 75, 76, 77, 81, 82, 84, 87, 90, 95, 98\n",
    "\n",
    "First, mark down the median, Q2, which in this case is the 10th value: 75.\n",
    "\n",
    "Q1 is the central point between the smallest score and the median. In this case, Q1 falls between the first and fifth score: 68. (Note that the median can also be included when calculating Q1 or Q3 for an odd set of values. If we were to include the median on either side of the middle point, then Q1 will be the middle value between the first and 10th score, which is the average of the fifth and sixth score—(fifth + sixth)/2 = (68 + 69)/2 = 68.5).\n",
    "\n",
    "Q3 is the middle value between Q2 and the highest score: 84. (Or if you include the median, Q3 = (82 + 84)/2 = 83).\n",
    "\n",
    "Now that we have our quartiles, let’s interpret their numbers. A score of 68 (Q1) represents the first quartile and is the 25th percentile. 68 is the median of the lower half of the score set in the available data—that is, the median of the scores from 59 to 75.\n",
    "\n",
    "Q1 tells us that 25% of the scores are less than 68 and 75% of the class scores are greater. Q2 (the median) is the 50th percentile and shows that 50% of the scores are less than 75, and 50% of the scores are above 75. Finally, Q3, the 75th percentile, reveals that 25% of the scores are greater and 75% are less than 84.\n",
    "<br>\n",
    "\n",
    "### What Is the Inter quartile Range of a Data Set?\n",
    "The inter quartile range is the middle 50% of measurements in a data set—in other words, the range of data between the upper quartile and the lower quartile. This is more statistically meaningful than using the full range of data, because it omits possible outliers.\n",
    "\n",
    "The following code cell demonstrates how to use extract quartiles from the data set `numbers = [59, 60, 65, 65, 68, 69, 70, 72, 75, 75, 76, 77, 81, 82, 84, 87, 90, 95, 98 ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numbers = np.array([59, 60, 65, 65, 68, 69, 70, 72, 75, 75, 76, 77, 81, 82, 84, 87, 90, 95, 98])\n",
    "\n",
    "# first quartile(25th percentile)\n",
    "q1 = np.percentile(numbers, 25)\n",
    "\n",
    "# second quartile (50th percentile/median)\n",
    "q2 = np.percentile(numbers, 50)\n",
    "\n",
    "# third quartile (75th percentile)\n",
    "q3 = np.percentile(numbers, 75)\n",
    "\n",
    "print(\"Q1:\", q1)\n",
    "print(\"Q2:\", q2)\n",
    "print(\"Q3:\", q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe0de4",
   "metadata": {},
   "source": [
    "## 8. Percentiles\n",
    "#### What is a Percentile:\n",
    "Percentiles are used to understand and interpret data. It is defined as the value to which a given percentage falls under.\n",
    "\n",
    "#### What is the formula for Percentile\n",
    "Percentiles can be calculated using the formula n = (P/100) x N, where P = percentile, N = number of values in a data set (sorted from smallest to largest), and n = ordinal rank of a given value.\n",
    "\n",
    "#### How is Percentile Calculated\n",
    "Here are a few steps to use the percentile formula to find the percentile. If q is any number between zero and hundred, the qth percentile is a value that divides the data into two parts i.e. the lowest part contains the q percent of the data and the rest of the data is the upper part.\n",
    "\n",
    "1.   Step 1: Arrange the data set in ascending order\n",
    "2.  Step 2: Count the number of values in the data set and represent it as r\n",
    "3.   Step 3: Calculate the value of q/100\n",
    "4.   Step 4: Multiply q percent by r\n",
    "5. Step 5: If the answer is not a whole number then rounding the number is required. If it is a whole number, continue to the next step.\n",
    "6.   Step 6: Count the values in the data set, and find the mean and the next number. The answer is the qth percentile\n",
    "7.   Step 7: Count the value in the data set, once you reach that number according to what we obtained in step 5 that is the qth percentile.\n",
    "\n",
    "#### Calculating Percentiles using values in a Data set\n",
    "\n",
    "The three definitions that define the k’th percentile are:\n",
    "The smallest value that is greater than k percent of the values.\n",
    "The smallest value that is greater than or equal to k percent of values.\n",
    "An interpolated value between the two closest ranks.\n",
    "\n",
    "To calculate percentiles using these three approaches, start by ranking your dataset from the lowest to the highest values.\n",
    "Let’s use these three methods with the following dataset (n = 11) to find the 70th percentile.\n",
    "\n",
    "**Definition 1: Greater Than**\n",
    "Using the first definition, we need to find the value that is greater than 70% of the values, and there are 11 values. Take 70% of 11, which is 7.7. Then, round 7.7 up to 8. Using the first definition, the value for the 70th percentile must be greater than eight values. Consequently, we pick the 9th ranked value in the dataset, which is 40.\n",
    "\n",
    "**Definition 2: Greater Than or Equal To**\n",
    "Using the second definition, we need to find the value that is greater than or equal to 70% of the values. Thanks to the “equal to” portion of the definition, we can use the 8th ranked value, which is 35.\n",
    "Using the first two definitions, we have found two values for the 70% percentile—35 and 40.\n",
    "\n",
    "**Definition 3: Using an Interpolation Approach**\n",
    "As you saw above, using either “greater” or “greater than or equal to” changes the results. Depending on the nature and size of your dataset, this difference can be substantial. Consequently, a third approach interpolates between two data values.\n",
    "To calculate an interpolated percentile, do the following:\n",
    "Calculate the rank to use for the percentile. Use: rank = p(n+1), where p = the percentile and n = the sample size. For our example, to find the rank for the 70th percentile, we take 0.7*(11 + 1) = 8.4.\n",
    "If the rank in step 1 is an integer, find the data value that corresponds to that rank and use it for the percentile.\n",
    "If the rank is not an integer, you need to interpolate between the two closest observations. For our example, 8.4 falls between 8 and 9, which corresponds to the data values of 35 and 40.\n",
    "Take the difference between these two observations and multiply it by the fractional portion of the rank. For our example, this is: (40 – 35)0.4 = 2.\n",
    "Take the lower-ranked value in step 3 and add the value from step 4 to obtain the interpolated value for the percentile. For our example, that value is 35 + 2 = 37.\n",
    "Using three common calculations for percentiles, we find three different values for the 70th percentile: 35, 37, and 40.\n",
    "<br>\n",
    "\n",
    "#### The syntax for finding Numpy Percentile:\n",
    "`numpy.percentile(a, q, axis=None, out=None, overwrite_input=False, interpolation='linear', keepdims=False)`\n",
    "\n",
    "The general parameters are the following.\n",
    "- `a`: Your input array.\n",
    "- `q`: Percentile or sequence of percentiles to compute. It should be between 0 and 100 inclusive.\n",
    "- `axis`: Axis or axes along the percentiles should be calculated.\n",
    "- `out`: Alternative output array in which to place the result.\n",
    "- `overwrite_input`: If True. It allows the input array to be modified by intermediate calculations.\n",
    "\n",
    "#### Example of implementation of NumPy percentile\n",
    "Note: You must import the numpy module for it to function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009bc86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example- Calculating the percentile for 1D Numpy array:\n",
    "# We will be calculating a single dimension array and use np.percentile() method on it.\n",
    "\n",
    "array_1d = np.array([1, 2, 3, 4, 9, 12])\n",
    "print(np.percentile(array_1d, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e60160",
   "metadata": {},
   "source": [
    "**Output**:\n",
    "For this example the output gotten was 3.5 as  the 50% percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0eb69e",
   "metadata": {},
   "source": [
    "\n",
    "#### Applications of Percentile\n",
    "1. Test scores\n",
    "2. Biometric measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49676a0",
   "metadata": {},
   "source": [
    "## 9. Covariance and Pearson’s Rank Correlation\n",
    "### Covariance\n",
    "Covariance represents the ability of two variables to increase or decrease together with respect to their mean values.\n",
    "Covariance can be positive, negative, or zero. Positive Covariance indicates that if one of the two variables has an increment in its values, the other variables’ values also increase.\n",
    "Negative Covariance indicates that if one of the two variables has an increment in its values, the other variables’ values decrease.\n",
    "Zero covariance indicates there is no linear relationship between the two variables.\n",
    "\n",
    "The following code cell demonstrates how to calculate the covariance of two quantitative variables stored as columns in a pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b35ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\"hours_of_practice\": [1, 5, 7, 2, 19, 67], \"SAT Score\": [970, 1230, 1000, 1200, 1490, 1590]})\n",
    "print(data)\n",
    "print(data.cov())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a6184f",
   "metadata": {},
   "source": [
    "Each column yields a positive covariance with the other columns. This indicates that when `hours of practice` is high, `SAT score` is high, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e7d1b",
   "metadata": {},
   "source": [
    "\n",
    "### Correlation\n",
    "Correlation is a normalized version of covariance. Covariance is a quantity that depends on the units of the variables used. Thus, if the units of either or both of the variables are changed, the digits of covariance change too. For solving this problem of units, correlation was introduced. Correlation can be positive, negative, or neutral.\n",
    "\n",
    "**Correlation coefficients** reflect the strength of the linear relationship between the two variables and the direction of their linear trend. In comparison, covariance coefficients are only used to highlight the direction (positive, negative, or neutral) of a linear relationship between the two variables. That is mainly because the correlation coefficient values are normalised using the product of standard deviations.\n",
    "<br>\n",
    "\n",
    "#### Pearson’s Rank correlation\n",
    "The Pearson correlation coefficient (r) is the most common way of measuring a linear correlation. It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables.\n",
    "The Pearson correlation method is the most common method to use for numerical variables; it assigns a value between − 1 and 1, where 0 is no correlation, 1 is total positive correlation, and − 1 is total negative correlation. This is interpreted as follows: a correlation value of 0.7 between two variables would indicate that a significant and positive relationship exists between the two. A positive correlation signifies that if variable A goes up, then B will also go up, whereas if the value of the correlation is negative, then if A increases, B decreases.\n",
    "A correlation can be calculated between two numerical values (e.g., age and salary) or between two category values (e.g., type of product and profession). As well as the correlation, the covariance of two variables is often calculated. In contrast with the correlation value, which must be between − 1 and 1, the covariance may assume any numerical value. The covariance indicates the grade of synchronization of the variance (or volatility) of the two variables.\n",
    "\n",
    "The following code cell demonstrates how to calculate the correlation of two numerical variables in a pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e991a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\"hours_of_practice\": [1, 5, 7, 2, 19, 67], \"SAT Score\": [970, 1230, 1000, 1200, 1490, 1590]})\n",
    "print(data)  # display the data set\n",
    "print(data.corr())  # calculate correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a26891",
   "metadata": {},
   "source": [
    "The results indicate that there is a correlation of approximately 80 % between hours studied and SAT Score. Therefore, the more hours studied, the higher the SAT score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9b7b4",
   "metadata": {},
   "source": [
    "## 10. HANDLING MISSING VALUES\n",
    "In working with a data set, one might encounter that some values are present whereas few are missing.\n",
    "\n",
    "There are 2 primary ways of handling missing values:\n",
    "- Deleting the Missing values\n",
    "- Imputing the Missing Values\n",
    "\n",
    "#### Deleting the Missing value\n",
    "Generally, this approach is not recommended. It is one of the quick and dirty techniques one can use to deal with missing values.\n",
    "It involves first identifying all observations in the data set containing missing values and removing them.\n",
    "In pandas DataFrames, this is accomplished using the `dropna()` DataFrame method.\n",
    "\n",
    "The following code cell demonstrates this approach for a data set containing student names and their grades with some students having missing grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Before deletion\n",
    "std_grades = pd.DataFrame([[\"Ekyi\", 100], [\"Fritz\"], [\"Rose\", 95], [\"Maria\", 28], [\"Sheena\"]])\n",
    "print(\"With Missing Values:\\n--------------------\\n\",std_grades)\n",
    "\n",
    "# Deleting missing values\n",
    "std_grades.dropna(inplace=True)\n",
    "\n",
    "# After deletion\n",
    "print(\"\\nWithout Missing Values:\\n------------------------\\n\", std_grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f5efc",
   "metadata": {},
   "source": [
    "As expected, the rows containing missing values were removed from the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426e49e",
   "metadata": {},
   "source": [
    "#### Imputing the Missing Value\n",
    "This method involves replacing all missing values.\n",
    "There are different ways of replacing the missing values. You can use the python libraries Pandas and Sci-kit learn as follows:\n",
    "\n",
    "1. **Replacing With Arbitrary Value**: If you can make an educated guess about the missing value then you can replace it with some arbitrary value using the following code.\n",
    "2. **Replacing With Mean**: This is the most common method of imputing missing values of numeric columns. If there are outliers then the mean will not be appropriate. In such cases, outliers need to be treated first.\n",
    "3. **Replacing With Mode**: Mode is the most frequently occurring value. It is used in the case of categorical features.\n",
    "4. **Replacing With Median**: Median is the middlemost value. It’s better to use the median value for imputation in the case of outliers.\n",
    "\n",
    "For pandas DataFrames, the `fillna()` method is used to replace all missing values in a DataFrame.\n",
    "The following code cell demonstrates all 4 aforementioned approaches for a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Before impute\n",
    "std_grades = pd.DataFrame([[\"Ekyi\", 100], [\"Fritz\"], [\"Rose\", 95], [\"Maria\", 28], [\"Sheena\"], [\"Ymir\", 95]])\n",
    "print(\"With Missing Values:\\n--------------------\\n\",std_grades)\n",
    "\n",
    "# Method 1: Arbitrary values\n",
    "arb_val = 8.5\n",
    "print(\"\\nMethod 1:\\n-----------------\\n\", std_grades.fillna(arb_val))  # replace all missing values with 8.5\n",
    "\n",
    "# Method 2: Mean\n",
    "print(\"\\nMethod 2:\\n-----------------\")\n",
    "mean = std_grades[1].mean()\n",
    "print(\"Mean:\", mean)\n",
    "print(\"\", std_grades.fillna(mean))  # replace all missing values with mean\n",
    "\n",
    "# Method 3: Mode\n",
    "print(\"\\nMethod 3:\\n-----------------\")\n",
    "mode = std_grades[1].mode()[0]\n",
    "print(\"Mode:\", mode)\n",
    "print(\"\", std_grades.fillna(mode))  # replace all missing values with mode\n",
    "\n",
    "# Method 4: Median\n",
    "print(\"\\nMethod 4:\\n-----------------\")\n",
    "median = std_grades[1].median()\n",
    "print(\"Median:\", median)\n",
    "print(\"\", std_grades.fillna(median))  # replace all missing values with median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9107d83",
   "metadata": {},
   "source": [
    "## 11. Detecting Outliers\n",
    "Outliers or anomalies in a data set refer to data values that are unusually separated from other values within the data set.\n",
    "For instance, in the data set `score = [1, 2, 3, 2, -1, 4, 5000]`, `5000` is an outlier as it is unusually larger than all other values within the data set.\n",
    "It does not follow the general trend of the data set, and can even be said to \"not belong\" to the data set.\n",
    "The scatter plot below visualizes the example data set above and helps to further clarify the nature of outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = pd.Series([1, 2, 3, 2, -1, 4, 5000])\n",
    "x_values = pd.Series([1, 2, 3, 4, 5, 6, 7])  # x-axis values for scatter plot.\n",
    "plt.scatter(scores.index, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffe577a",
   "metadata": {},
   "source": [
    "As can be seen in the figure above, the data point `5000` lies unusually far away from all other points in the data set. It is therefore an outlier or anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9893a6",
   "metadata": {},
   "source": [
    "In working with data for data science, outliers may occur in one's data set and the two main methods for detecting outliers are as follows:\n",
    "1. Using the Standard Deviation\n",
    "2. Visualizing the data set with a box plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388bcc7",
   "metadata": {},
   "source": [
    "### Using Standard Deviation\n",
    "As mentioned in section 3, the standard deviation of a data set measures how spread apart the data set is, (i.e. how far each value in the data set is from the mean).\n",
    "\n",
    "In statistics, a data value is considered to be an outlier if it is above or below **some number of standard deviations from the mean** of the data set.\n",
    "The number of standard deviations is typically referred to as the threshold.\n",
    "\n",
    "That is, if one calculates the standard deviation of a data set to be $s$ and sets the threshold for detecting outliers to be 3, any value that is smaller than the mean by more than $3s$ or larger than the mean by more than $3s$ will be considered an outlier.\n",
    "\n",
    "For instance, consider the following data set comprising salaries of 10 employees.\n",
    "`salary = [100, 150, 85, 200, 105, 65, 275, 250, 220, 1000]`\n",
    "\n",
    "By intuition, `1000` must definitely be an outlier.\n",
    "To prove this, we shall calculate the standard deviation and compare all values with it to detect those which are outliers.\n",
    "\n",
    "We first start by calculating the mean and standard deviation before comparing each data point as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "salary = pd.Series([100, 150, 85, 200, 105, 65, 275, 250, 220, 1000])\n",
    "\n",
    "# calculating mean\n",
    "total = 0\n",
    "N = len(salary)\n",
    "for amount in salary:\n",
    "    total += amount\n",
    "mean = total / N\n",
    "print(\"Mean:\", mean)\n",
    "\n",
    "# calculating standard deviation\n",
    "std_total = 0\n",
    "for amount in salary:\n",
    "    std_total += (amount - mean) ** 2\n",
    "std = sqrt(std_total / N)\n",
    "print(\"Standard Deviation:\", std)\n",
    "\n",
    "# comparing for outliers\n",
    "threshold = 2  # using a threshold of 2 standard deviations from mean\n",
    "for amount in salary:\n",
    "    is_outlier = False\n",
    "    if amount < (mean - threshold * std):\n",
    "        is_outlier = True\n",
    "    elif amount > (mean + threshold * std):\n",
    "        is_outlier = True\n",
    "    print(amount, is_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a67cc",
   "metadata": {},
   "source": [
    "As we predicted, the number 1000, has been revealed to be an outlier in our data set using a threshold of 2 standard deviations from the mean.\n",
    "\n",
    "Note that, by default, the threshold for detecting outliers is usually set to 3 standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a9dd4",
   "metadata": {},
   "source": [
    "### Visualizing the Data Set with a Box Plot\n",
    "A box plot is a special type of plot comprising a rectangular box with whiskers that provides a visual summary of a data set using 5 distinct numbers, namely\n",
    " - The Minimum\n",
    " - The 1st Quartile\n",
    " - The 2nd Quartile (Median)\n",
    " - The 3rd Quartile\n",
    " - The Maximum\n",
    "\n",
    "**Parts of a Box Plot**\n",
    "<img alt=\"parts of a box plot\" src=\"boxplot.png\" title=\"Parts of a Box Plot\"/>\n",
    "\n",
    "The Minimum is simply the smallest value within the data set.\n",
    "The Maximum is simply the largest value within the data set.\n",
    "\n",
    "If the data set is to be divided into 4 equal parts or quarters,\n",
    "The 1st Quartile is simply the value that would separate the 1st and 2nd quarter.\n",
    "The 2nd Quartile is simply the value that would separate the 2nd and 3rd quarter.\n",
    "The 3rd Quartile is simply the value that would separate the 3rd and 4th quarter.\n",
    "\n",
    "The data set must be sorted first before any of these 5 numbers can be calculated to provide a summary of the data.\n",
    "\n",
    "The difference between the 1st quartile and the 3rd quartile is a special number referred to as the InterQuartile Range(IQR).\n",
    "This number is used to identify outliers in a data set.\n",
    "Specifically, outliers in a data set are values that are either\n",
    " - below $Q1 - 1.5 \\times IQR$\n",
    " - above $Q3 + 1.5 \\times IQR$\n",
    "\n",
    "Consider the following data set: `heights = [17.0, 18.5, 22.0, 12.3, 14.7, 19.1, 20.0, 13.5, 1, 43]`\n",
    "Clearly the heights `43` and `1` are outliers.\n",
    "The code cell below constructs a box plot to prove that they are indeed outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dce7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "heights = pd.Series([17.0, 18.5, 22.0, 12.3, 14.7, 19.1, 20.0, 13.5, 1, 43])\n",
    "plt.boxplot(heights, vert=False)  # construct a horizontal boxplot using heights\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd6419",
   "metadata": {},
   "source": [
    "As expected, the marker representing the data points for `1` and `43` have been separated from the box plot, indicating that they are outliers or anomalies.\n",
    "\n",
    "Note that, the box plot's minimum and maximum don't actually represent the minimum and maximum values of the data set (which are the outliers).\n",
    "They however represent the minimum and maximum of the data set after the removal of the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6303cd0",
   "metadata": {},
   "source": [
    "## 12. Handling Outliers\n",
    "Upon detecting outliers in a data set with either of the methods identified above, there are 3 methods through which one can handle outliers:\n",
    "1. Deleting outliers from the data set.\n",
    "2. Imputing with 10th and 90th percentile.\n",
    "3. Imputing with median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7df19",
   "metadata": {},
   "source": [
    "### Deleting outliers\n",
    "This method involves deleting all values that are identified as outliers from the data set.\n",
    "\n",
    "**Deleting outliers detected via Boxplot**\n",
    "If the outliers in the data set were identified using a box plot, the `plt.boxplot()` function returns a dictionary whose `fliers` key contains the outliers of the data set.\n",
    "\n",
    "The following code cell retrieves outliers from a horizontal boxplot and removes them from the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "heights = [17.0, 18.5, 22.0, 12.3, 14.7, 19.1, 20.0, 13.5, 1, 43, 78]\n",
    "print(\"Data set with outliers:\", heights)\n",
    "\n",
    "# construct horizontal boxplot\n",
    "bplot = plt.boxplot(heights, vert=False)\n",
    "\n",
    "# retrieve outliers\n",
    "outliers = bplot[\"fliers\"][0].get_xdata()\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "# remove outliers\n",
    "for outlier in outliers:\n",
    "    heights.remove(outlier)\n",
    "\n",
    "print(\"Data set without outliers:\", heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5aa78e",
   "metadata": {},
   "source": [
    "The following code cell accomplishes the same with a vertical boxplot instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9615fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "heights = [17.0, 18.5, 22.0, 12.3, 14.7, 19.1, 20.0, 13.5, 1, 43, 78]\n",
    "print(\"Data set with outliers:\", heights)\n",
    "\n",
    "# construct horizontal boxplot\n",
    "bplot = plt.boxplot(heights, vert=True)  # NB: vert=False -> vert=True\n",
    "\n",
    "# retrieve outliers\n",
    "outliers = bplot[\"fliers\"][0].get_ydata()  # NB: .get_xdata() -> .get_ydata()\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "# remove outliers\n",
    "for outlier in outliers:\n",
    "    heights.remove(outlier)\n",
    "\n",
    "print(\"Data set without outliers:\", heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa33841",
   "metadata": {},
   "source": [
    "A similar approach to this can be used to delete outliers detected using the standard deviation.\n",
    "Values that compare as greater than or less than the mean by more than `threshold * standard deviation` will be deleted from the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cd69c",
   "metadata": {},
   "source": [
    "### Imputing with 10th and 90th percentiles\n",
    "Another way of dealing with outliers is by imputing the value of the 10th and 90th percentile to all values less than the 10th percentile or greater than the 90th percentile respectively.\n",
    "\n",
    "The following code snippet demonstrates this for a data set `temperatures = [37, 28, 33, 35, 18, -5, 89, 34, -12, 40]` whose outliers are `[-12, 40]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "temperatures = [37, 28, 33, 35, 18, 9, 89, 34, -12, 40]\n",
    "\n",
    "# calculating percentiles\n",
    "percentile10 = np.percentile(temperatures, 10)\n",
    "percentile90 = np.percentile(temperatures, 90)\n",
    "\n",
    "# box plot before imputation\n",
    "plt.boxplot(temperatures, vert=False)\n",
    "plt.title(\"Before Imputation\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# imputing outliers\n",
    "for i in range(len(temperatures)):\n",
    "    if temperatures[i] < percentile10:\n",
    "        temperatures[i] = percentile10\n",
    "    elif temperatures[i] > percentile90:\n",
    "        temperatures[i] = percentile90\n",
    "    i+= 1\n",
    "\n",
    "\n",
    "\n",
    "# box plot after imputation\n",
    "plt.boxplot(temperatures, vert=False)\n",
    "plt.title(\"After Imputation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e3aa1",
   "metadata": {},
   "source": [
    "As expected, imputing the 10th and 90th percentiles removed outliers from our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411db0b",
   "metadata": {},
   "source": [
    "### Imputing with Median\n",
    "Another way of getting rid of outliers in a given data set is by replacing all outliers with the median.\n",
    "In the following code cell, the outliers in the data set `scores = [1020, 1350, 1490, 1350, 1480, 1390, 1500, 1400, 890, 2400]` are replaced with the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "scores = [1020, 1350, 1490, 1350, 1480, 1390, 1500, 1400, 890, 2400]\n",
    "\n",
    "# calculating median\n",
    "median = np.median(scores)\n",
    "\n",
    "# boxplot before imputation\n",
    "bplot = plt.boxplot(scores, vert=False)\n",
    "plt.title(\"Before Imputation\")\n",
    "plt.show()\n",
    "\n",
    "# extracting outliers from boxplot\n",
    "outliers = bplot[\"fliers\"][0].get_xdata()\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "# imputing median\n",
    "for i in range(len(scores)):\n",
    "    if scores[i] in outliers:\n",
    "        scores[i] = median\n",
    "\n",
    "# boxplot after imputation\n",
    "plt.boxplot(scores, vert=False)\n",
    "plt.title(\"After Imputation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142e73c",
   "metadata": {},
   "source": [
    "As expected, the boxplot confirms that outliers have been removed from our data set after imputing outlier values were imputed with median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c0ebb",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> END OF NOTEBOOK</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
